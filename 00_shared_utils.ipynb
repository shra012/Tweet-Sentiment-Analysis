{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7144467",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Project Shared Utilities\n",
    "This notebook provides shared code (cleaning, feature engineering, and pipeline builders) that other notebooks will `%run`.\n",
    "\n",
    "**How to use:** In any notebook, run:\n",
    "```python\n",
    "%run ./00_shared_utils.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca6194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q scikit-learn imbalanced-learn xgboost shap emoji\n",
    "%pip install -q sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7074c2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared utilities loaded. Use build_full_pipeline(classifier) to create a text model pipeline.\n"
     ]
    }
   ],
   "source": [
    "import re, html, json, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from scipy import sparse\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "def set_seeds(seed=RANDOM_STATE):\n",
    "    import os, random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "_contractions = {\n",
    "    \"can't\": \"can not\", \"won't\": \"will not\", \"n't\": \" not\",\n",
    "    \"i'm\": \"i am\", \"it's\": \"it is\", \"that's\": \"that is\",\n",
    "    \"what's\": \"what is\", \"there's\": \"there is\", \"i've\": \"i have\",\n",
    "    \"you're\": \"you are\", \"they're\": \"they are\", \"we're\": \"we are\",\n",
    "    \"i'll\": \"i will\", \"you'll\": \"you will\", \"they'll\": \"they will\",\n",
    "    \"i'd\": \"i would\", \"you'd\": \"you would\", \"they'd\": \"they would\"\n",
    "}\n",
    "\n",
    "url_re = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "mention_re = re.compile(r\"@\\w+\")\n",
    "hashtag_re = re.compile(r\"#(\\w+)\")\n",
    "elong_re = re.compile(r\"(.)\\1{2,}\")\n",
    "ws_re = re.compile(r\"\\s+\")\n",
    "\n",
    "emoji_re = re.compile(r\"[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F1E0-\\U0001F1FF]\", flags=re.UNICODE)\n",
    "\n",
    "def expand_contractions(text: str) -> str:\n",
    "    t = text\n",
    "    for k, v in _contractions.items():\n",
    "        t = re.sub(re.escape(k), v, t, flags=re.IGNORECASE)\n",
    "    return t\n",
    "\n",
    "def normalize_text(t: str) -> Dict[str, Any]:\n",
    "    if not isinstance(t, str):\n",
    "        t = \"\" if t is None else str(t)\n",
    "    raw = t\n",
    "    t = t.strip().lower()\n",
    "    t = html.unescape(t)\n",
    "\n",
    "    url_count = len(url_re.findall(t))\n",
    "    t = url_re.sub(\" \", t)\n",
    "\n",
    "    mention_count = len(mention_re.findall(t))\n",
    "    t = mention_re.sub(\" \", t)\n",
    "\n",
    "    hashtags = hashtag_re.findall(t)\n",
    "    hashtag_count = len(hashtags)\n",
    "    t = hashtag_re.sub(lambda m: \" \" + m.group(1) + \" \", t)\n",
    "\n",
    "    has_emoji = 1 if emoji_re.search(t) else 0\n",
    "\n",
    "\n",
    "    t = expand_contractions(t)\n",
    "    t = elong_re.sub(r\"\\1\\1\", t)\n",
    "\n",
    "    t = re.sub(r\"[^a-z0-9\\s!\\?\\.,' ]+\", \" \", t)\n",
    "    t = ws_re.sub(\" \", t).strip()\n",
    "\n",
    "    return {\n",
    "        \"text_clean\": t,\n",
    "        \"url_count\": url_count,\n",
    "        \"mention_count\": mention_count,\n",
    "        \"hashtag_count\": hashtag_count,\n",
    "        \"has_emoji\": has_emoji,\n",
    "        \"text_len\": len(t),\n",
    "        \"raw\": raw\n",
    "    }\n",
    "\n",
    "class CleaningTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Transforms a DataFrame with columns ['text', 'selected_text'] to a new DataFrame\n",
    "    containing 'text_clean' and numeric features including keyword overlap.\n",
    "\n",
    "    Expects columns: 'text' and optionally 'selected_text'.\n",
    "\n",
    "    Returns a pandas DataFrame for downstream selectors.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            X = X.to_frame(name=\"text\")\n",
    "        df = X.copy()\n",
    "        if 'text' not in df.columns:\n",
    "            for c in df.columns:\n",
    "                if 'text' in c.lower():\n",
    "                    df = df.rename(columns={c:'text'})\n",
    "                    break\n",
    "        results = [normalize_text(t) for t in df['text'].fillna(\"\")]\n",
    "        out = pd.DataFrame(results, index=df.index)\n",
    "        if 'selected_text' in df.columns:\n",
    "            sel = df['selected_text'].fillna(\"\").str.lower().str.split()\n",
    "            txt = out['text_clean'].fillna(\"\").str.split()\n",
    "            overlap = []\n",
    "            for s, tks in zip(sel, txt):\n",
    "                if not s or not tks:\n",
    "                    overlap.append(0.0)\n",
    "                else:\n",
    "                    sset = set(s)\n",
    "                    count = sum(1 for w in tks if w in sset)\n",
    "                    overlap.append(count / (len(s) + 1e-9))\n",
    "            out['keyword_overlap'] = overlap\n",
    "        else:\n",
    "            out['keyword_overlap'] = 0.0\n",
    "        return out\n",
    "\n",
    "def _select_single_column(df, name):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        column = df[name]\n",
    "        return column.values if isinstance(column, pd.Series) else column\n",
    "    return df\n",
    "\n",
    "def _select_multi_columns(df, names):\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        return df[names].values\n",
    "    return df\n",
    "\n",
    "def select_col(name):\n",
    "    return FunctionTransformer(_select_single_column, kw_args={'name': name}, validate=False)\n",
    "\n",
    "def select_cols(names):\n",
    "    return FunctionTransformer(_select_multi_columns, kw_args={'names': names}, validate=False)\n",
    "\n",
    "class ToSparse(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        X = np.asarray(X)\n",
    "        return sparse.csr_matrix(X)\n",
    "\n",
    "def build_feature_union(word_ng=(1,2), char_ng=(3,5), min_df=3):\n",
    "    word = Pipeline([\n",
    "        ('sel', select_col('text_clean')),\n",
    "        ('tfidf', TfidfVectorizer(ngram_range=word_ng, min_df=min_df))\n",
    "    ])\n",
    "    char = Pipeline([\n",
    "        ('sel', select_col('text_clean')),\n",
    "        ('tfidf', TfidfVectorizer(analyzer='char', ngram_range=char_ng, min_df=min_df))\n",
    "    ])\n",
    "    numeric = Pipeline([\n",
    "        ('sel', select_cols(['url_count','mention_count','hashtag_count','has_emoji','text_len','keyword_overlap'])),\n",
    "        ('tosparse', ToSparse()),\n",
    "        ('scale', MaxAbsScaler())\n",
    "    ])\n",
    "    return FeatureUnion([\n",
    "        ('word', word),\n",
    "        ('char', char),\n",
    "        ('num', numeric)\n",
    "    ])\n",
    "\n",
    "def build_full_pipeline(classifier):\n",
    "    return Pipeline([\n",
    "        ('clean', CleaningTransformer()),\n",
    "        ('features', build_feature_union()),\n",
    "        ('clf', classifier)\n",
    "    ])\n",
    "\n",
    "print(\"Shared utilities loaded. Use build_full_pipeline(classifier) to create a text model pipeline.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}